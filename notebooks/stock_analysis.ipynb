{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stock Analysis Project\n",
        "## Historical Price Analysis, Returns, and Risk Metrics\n",
        "\n",
        "This notebook provides a comprehensive analysis of historical stock price data, computing key financial metrics including returns, volatility, Sharpe ratios, and drawdowns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Data Acquisition\n",
        "\n",
        "First, we'll import necessary libraries and fetch historical price data for multiple stocks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Add src directory to path - handle both notebook and script execution\n",
        "if os.path.basename(os.getcwd()) == 'notebooks':\n",
        "    # We're in the notebooks directory\n",
        "    project_root = os.path.dirname(os.getcwd())\n",
        "else:\n",
        "    # We're in the project root\n",
        "    project_root = os.getcwd()\n",
        "\n",
        "sys.path.insert(0, os.path.join(project_root, 'src'))\n",
        "\n",
        "from data_loader import fetch_prices, save_raw_data\n",
        "from indicators import prepare_prices, max_drawdown\n",
        "from analysis import (\n",
        "    compute_daily_returns,\n",
        "    compute_cumulative_returns,\n",
        "    annual_metrics,\n",
        "    rolling_volatility_annualized,\n",
        "    prepare_returns,\n",
        "    compute_summary_stats\n",
        ")\n",
        "from visualizations import (\n",
        "    plot_equity_curves,\n",
        "    plot_performance_table,\n",
        "    plot_rolling_volatility,\n",
        "    plot_max_drawdown,\n",
        "    plot_sharpe_ratios,\n",
        "    create_summary_charts\n",
        ")\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Fetch Historical Price Data\n",
        "\n",
        "We'll analyze 5 major tech stocks: Apple (AAPL), Microsoft (MSFT), Google (GOOGL), Amazon (AMZN), and Tesla (TSLA).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define tickers and date range\n",
        "tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']\n",
        "start_date = '2018-01-01'\n",
        "end_date = None  # None means up to today\n",
        "\n",
        "print(f\"Fetching data for {len(tickers)} stocks: {', '.join(tickers)}\")\n",
        "print(f\"Date range: {start_date} to {'Today' if end_date is None else end_date}\")\n",
        "\n",
        "# Fetch price data\n",
        "prices_raw = fetch_prices(tickers, start=start_date, end=end_date)\n",
        "\n",
        "print(f\"\\nRaw data shape: {prices_raw.shape}\")\n",
        "print(f\"Date range: {prices_raw.index.min()} to {prices_raw.index.max()}\")\n",
        "print(f\"\\nFirst 5 rows:\")\n",
        "print(prices_raw.head())\n",
        "print(f\"\\nLast 5 rows:\")\n",
        "print(prices_raw.tail())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Save Raw Data\n",
        "\n",
        "Let's save the raw data for future reference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save raw data\n",
        "data_dir = os.path.join(project_root, 'data')\n",
        "save_raw_data(prices_raw, output_dir=data_dir, filename='raw_prices.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Cleaning and Preparation\n",
        "\n",
        "We'll clean the data by handling missing values and ensuring proper alignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean and prepare prices\n",
        "prices_clean = prepare_prices(prices_raw)\n",
        "\n",
        "print(f\"Clean prices shape: {prices_clean.shape}\")\n",
        "print(f\"\\nMissing values per column:\")\n",
        "print(prices_clean.isna().sum())\n",
        "print(f\"\\nPercentage of missing values:\")\n",
        "print((prices_clean.isna().sum() / len(prices_clean) * 100).round(2))\n",
        "\n",
        "# Save clean prices\n",
        "prices_clean.to_csv(os.path.join(data_dir, 'clean_prices.csv'))\n",
        "print(f\"\\nClean prices saved to data/clean_prices.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Core Analytics: Returns and Cumulative Returns\n",
        "\n",
        "We'll compute daily returns and cumulative returns using Pandas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute returns\n",
        "daily_returns = compute_daily_returns(prices_clean)\n",
        "cum_returns = compute_cumulative_returns(daily_returns)\n",
        "\n",
        "print(\"Daily Returns Statistics:\")\n",
        "print(daily_returns.describe().round(4))\n",
        "print(f\"\\nDaily Returns (last 5 days):\")\n",
        "print(daily_returns.tail())\n",
        "print(f\"\\nCumulative Returns (last 5 days):\")\n",
        "print(cum_returns.tail())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Validation: Cumulative Returns\n",
        "\n",
        "Let's validate that cumulative returns are computed correctly using NumPy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validation: Compute cumulative return using np.prod\n",
        "for ticker in tickers:\n",
        "    if ticker in daily_returns.columns:\n",
        "        arr = daily_returns[ticker].values\n",
        "        arr = arr[~np.isnan(arr)]\n",
        "        cum_return_np = np.prod(1 + arr) - 1\n",
        "        cum_return_pandas = cum_returns[ticker].iloc[-1]\n",
        "        \n",
        "        print(f\"{ticker}:\")\n",
        "        print(f\"  NumPy cumulative return: {cum_return_np:.4f}\")\n",
        "        print(f\"  Pandas cumulative return: {cum_return_pandas:.4f}\")\n",
        "        print(f\"  Difference: {abs(cum_return_np - cum_return_pandas):.8f}\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Annualized Metrics (NumPy Implementation)\n",
        "\n",
        "We'll compute annualized return, annualized volatility, and Sharpe ratio using NumPy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute annualized metrics\n",
        "risk_free_rate = 0.0  # Can be updated to actual risk-free rate\n",
        "stats = annual_metrics(daily_returns, rf=risk_free_rate)\n",
        "\n",
        "# Add max drawdown\n",
        "max_dd = max_drawdown(prices_clean)\n",
        "stats['max_drawdown'] = max_dd\n",
        "\n",
        "# Format for display\n",
        "stats_display = stats.copy()\n",
        "for col in ['annual_return', 'annual_vol', 'total_return', 'max_drawdown']:\n",
        "    if col in stats_display.columns:\n",
        "        stats_display[col] = stats_display[col].apply(lambda x: f\"{x:.2%}\")\n",
        "\n",
        "print(\"Summary Statistics:\")\n",
        "print(\"=\" * 80)\n",
        "print(stats_display)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\nDetailed Statistics (numeric):\")\n",
        "print(stats)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Performance Ranking\n",
        "\n",
        "Let's identify the top performers across different metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"TOP PERFORMER (Highest Annual Return):\")\n",
        "top_return = stats['annual_return'].idxmax()\n",
        "print(f\"  {top_return}: {stats.loc[top_return, 'annual_return']:.2%}\\n\")\n",
        "\n",
        "print(\"LOWEST VOLATILITY (Most Stable):\")\n",
        "lowest_vol = stats['annual_vol'].idxmin()\n",
        "print(f\"  {lowest_vol}: {stats.loc[lowest_vol, 'annual_vol']:.2%}\\n\")\n",
        "\n",
        "print(\"BEST SHARPE RATIO (Best Risk-Adjusted Return):\")\n",
        "best_sharpe = stats['sharpe'].idxmax()\n",
        "print(f\"  {best_sharpe}: {stats.loc[best_sharpe, 'sharpe']:.4f}\\n\")\n",
        "\n",
        "print(\"WORST MAX DRAWDOWN (Largest Decline):\")\n",
        "worst_dd = stats['max_drawdown'].idxmin()\n",
        "print(f\"  {worst_dd}: {stats.loc[worst_dd, 'max_drawdown']:.2%}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Rolling Statistics\n",
        "\n",
        "We'll compute rolling mean and rolling volatility over a 20-day window.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute rolling statistics\n",
        "window = 20\n",
        "rolling_mean_returns = daily_returns.rolling(window=window).mean()\n",
        "rolling_vol = rolling_volatility_annualized(daily_returns, window=window)\n",
        "\n",
        "print(f\"Rolling Mean Returns ({window}-day window) - Last 5 days:\")\n",
        "print(rolling_mean_returns.tail())\n",
        "print(f\"\\nRolling Volatility Annualized ({window}-day window) - Last 5 days:\")\n",
        "print(rolling_vol.tail())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualizations\n",
        "\n",
        "Let's create comprehensive charts to visualize the analysis results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory for charts\n",
        "slides_dir = os.path.join(project_root, 'slides')\n",
        "os.makedirs(slides_dir, exist_ok=True)\n",
        "\n",
        "# Generate all charts\n",
        "create_summary_charts(prices_clean, daily_returns, cum_returns, rolling_vol, stats, output_dir=slides_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Equity Curves\n",
        "\n",
        "The equity curves show cumulative returns over time, allowing us to compare performance across stocks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_equity_curves(cum_returns, title=\"Cumulative Returns (Equity Curves)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Performance Metrics\n",
        "\n",
        "Bar charts comparing annualized returns and volatility across stocks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_performance_table(stats, title=\"Performance Metrics Comparison\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Rolling Volatility\n",
        "\n",
        "Time-series of rolling volatility with shaded high-volatility periods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_rolling_volatility(rolling_vol, title=\"Rolling Volatility (20-day window, Annualized)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.4 Drawdown Analysis\n",
        "\n",
        "Visualization of drawdowns from peak values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_max_drawdown(prices_clean, title=\"Maximum Drawdown Analysis\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.5 Sharpe Ratio Comparison\n",
        "\n",
        "Comparison of risk-adjusted returns (Sharpe ratios) across stocks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_sharpe_ratios(stats, title=\"Sharpe Ratio Comparison (Risk-Adjusted Returns)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Findings and Interpretation\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "Based on our analysis of 5 major tech stocks from 2018 to present:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary findings\n",
        "print(\"=\" * 80)\n",
        "print(\"KEY FINDINGS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nAnalysis Period: {prices_clean.index.min().strftime('%Y-%m-%d')} to {prices_clean.index.max().strftime('%Y-%m-%d')}\")\n",
        "print(f\"Number of trading days: {len(prices_clean)}\")\n",
        "print(f\"\\nStocks Analyzed: {', '.join(tickers)}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"PERFORMANCE RANKINGS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Sort by annual return\n",
        "stats_sorted_return = stats.sort_values('annual_return', ascending=False)\n",
        "print(\"\\n1. ANNUALIZED RETURN (Top to Bottom):\")\n",
        "for idx, (ticker, row) in enumerate(stats_sorted_return.iterrows(), 1):\n",
        "    print(f\"   {idx}. {ticker}: {row['annual_return']:.2%}\")\n",
        "\n",
        "# Sort by Sharpe ratio\n",
        "stats_sorted_sharpe = stats.sort_values('sharpe', ascending=False)\n",
        "print(\"\\n2. SHARPE RATIO (Best Risk-Adjusted Returns):\")\n",
        "for idx, (ticker, row) in enumerate(stats_sorted_sharpe.iterrows(), 1):\n",
        "    print(f\"   {idx}. {ticker}: {row['sharpe']:.4f}\")\n",
        "\n",
        "# Sort by volatility\n",
        "stats_sorted_vol = stats.sort_values('annual_vol', ascending=True)\n",
        "print(\"\\n3. VOLATILITY (Lowest to Highest):\")\n",
        "for idx, (ticker, row) in enumerate(stats_sorted_vol.iterrows(), 1):\n",
        "    print(f\"   {idx}. {ticker}: {row['annual_vol']:.2%}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"METHODOLOGY\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\"\"\n",
        "1. Data Acquisition: Fetched historical adjusted close prices using yfinance\n",
        "2. Data Cleaning: Forward-filled missing values and aligned trading days\n",
        "3. Returns Calculation: \n",
        "   - Daily returns: r_t = (P_t / P_{t-1}) - 1\n",
        "   - Cumulative returns: computed as (1 + daily_returns).cumprod() - 1\n",
        "4. Annualized Metrics (assuming 252 trading days/year):\n",
        "   - Annualized Return: mean_daily_return * 252\n",
        "   - Annualized Volatility: std_daily_return * sqrt(252)\n",
        "   - Sharpe Ratio: (annualized_return - risk_free_rate) / annualized_volatility\n",
        "5. Rolling Statistics: 20-day rolling window for mean and volatility\n",
        "6. Risk Metrics: Maximum drawdown from peak values\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SUMMARY TABLE\")\n",
        "print(\"=\" * 80)\n",
        "print(stats.round(4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Export Summary Statistics\n",
        "\n",
        "Save the summary statistics to CSV for external analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save summary statistics\n",
        "stats.to_csv(os.path.join(data_dir, 'summary_stats.csv'))\n",
        "print(f\"Summary statistics saved to data/summary_stats.csv\")\n",
        "stats.head()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
